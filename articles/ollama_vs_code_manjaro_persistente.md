#  Ollama en Manjaro Persistente y VS Code (con GPU AMD)

Tras los dos intentos previos (Debian 12 sin 茅xito, y Manjaro en Live USB con errores de espacio), se abord贸 una tercera estrategia: instalar **Manjaro XFCE de forma persistente en un pendrive de 32 GB**, utiliz谩ndolo como un disco duro real para ejecutar modelos LLM con **aceleraci贸n GPU** en local.

---

### О Instalaci贸n de Manjaro en el Pendrive (Modo Persistente)

La clave para obtener persistencia real no era simplemente flashear la ISO, sino realizar una instalaci贸n completa del sistema en el pendrive:

1. Se flashe贸 la **ISO de Manjaro XFCE** en un **segundo pendrive** para crear un Live USB de instalaci贸n.
2. Se arranc贸 el PC desde ese Live USB e inici贸 el instalador **Calamares**.
3. Se seleccion贸 cuidadosamente el **pendrive de 32 GB como disco de destino**, exactamente como se har铆a con un disco interno.

 **Resultado:** Un entorno Manjaro funcional y persistente, con acceso total al almacenamiento, drivers modernos y todos los recursos del sistema, eliminando as铆 las limitaciones del modo Live.

---

###  Ollama con Aceleraci贸n GPU

Ya con Manjaro persistente corriendo, se instalaron los paquetes necesarios:

- `curl`, `lm_sensors`, `radeontop`
- Ollama (`curl -fsSL https://ollama.com/install.sh | sh`)

Al ejecutar `ollama run phi3:mini`, se detect贸 por fin el **stack ROCm activo**, habilitando el uso de la **Radeon RX Vega 56** como motor de inferencia. Herramientas como `radeontop` y el comportamiento de los ventiladores confirmaron que la GPU se estaba usando efectivamente.

 **Status:** 隆xito! Ollama funcionando con aceleraci贸n GPU AMD en entorno local.

---

###  Integraci贸n en VS Code: Chat con LLM en local

Con el modelo funcionando en terminal, el objetivo pas贸 a integrarlo en el entorno de desarrollo:

1. **Primer intento fallido**: La extensi贸n "CodeGPT" estaba orientada a servicios en la nube y requer铆a autenticaci贸n en Google/GitHub.
2. **Soluci贸n adecuada**: Se identific贸 e instal贸 la extensi贸n **"Continue"**, compatible con servidores Ollama locales.

Una vez instalada, la extensi贸n detect贸 el modelo local (`llama3`) y pidi贸 confirmaci贸n para usarlo. No fue necesario descargar nada m谩s, y el chat se habilit贸 correctamente dentro de VS Code.

И **Resultado final:** Chat de Llama 3 operativo en el lateral de VS Code, listo para asistir en tareas de programaci贸n.

---

###  Conclusiones

锔 Este tercer intento valid贸 que es completamente viable ejecutar **Ollama + LLM con GPU AMD** en un entorno local si se cumplen ciertas condiciones:

- Utilizar una distro Linux actualizada (como **Manjaro**).
- Evitar los entornos Live para tareas pesadas y optar por instalaciones persistentes.
- Instalar la **extensi贸n correcta** en VS Code (como **Continue**) que se comunique con Ollama local.
- Asegurar que el stack **ROCm est茅 soportado** por la tarjeta gr谩fica y el kernel/distribuci贸n elegida.

---

###  Pr贸ximos Pasos

Con Ollama ya operativo, el siguiente paso ser谩 evaluar sus capacidades en tareas concretas: generaci贸n de c贸digo, sugerencias, refactorizaci贸n ligera y an谩lisis sint谩ctico, midiendo su utilidad real en proyectos web (HTML/CSS/JS).

Adem谩s, se plantea explorar otros modelos m谩s ligeros o espec铆ficos (como `codegemma` o `starcoder`) y refinar la instalaci贸n para que funcione desde SSD o incluso contenedor port谩til.

---
